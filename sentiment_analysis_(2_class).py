# -*- coding: utf-8 -*-
"""Sentiment Analysis (2 class)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BIxK77oN8zpoJJoM9o_0XNR_IeUx1oMW

## Import packages
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from tqdm import tqdm
from sklearn.metrics import classification_report

"""## Import Dataset"""

!gdown --id 1iC_2Bfnw2xp7x5xod4TfU6zUL8JYjkRM

df = pd.read_excel('100k digikala.xlsx')

df.shape

df.head()

df.drop(['product_id','likes','dislikes'], axis=1, inplace = True)

df.head()

df['title_en'].value_counts().sort_values()

df[df['title_en'] == 'IT']

df_ac = df[(df['title_en'] == 'AC') | (df['title_en'] == 'MO') |  (df['title_en'] == 'IT')]

df_ac.drop(['product_title','title_en','user_id','verification_status','title','advantages','disadvantages'], axis=1 ,inplace=True)

df_ac.head()

df_ac.shape

df_ac['recommend'].value_counts().sort_values()

df_ac = df_ac[(df_ac['recommend'] == 'not_recommended') | (df_ac['recommend'] =='recommended')]

df_ac.shape

df_ac.head()

plt.bar(['recommended','not_recommended'], df_ac['recommend'].value_counts())

df_ac['comment'].iloc[11]

"""## hazm"""

#hazm
!pip install hazm
from hazm import *

stemmer = Stemmer()
lemma = Lemmatizer()
normalizer = Normalizer()

stopword = stopwords_list()
unwanted_num = {'زیاد','هستند','عالی','نیستند','نبود','نمی شود', 'است', 'می شود', 'شد', 'شده', 'بود' ,'نیست', 'خوبی', 'خوب' ,'بهترین', 'بهتر', 'متاسفانه'}
stopwords = [item for item in stopword if item not in unwanted_num]

stopword

"""## preprocessing"""

def preprocess(review_text):
  review_text = re.sub('https\S+','',review_text)
  review_text = re.sub('[a-zA-z]','',review_text)
  review_text = re.sub(r'[!”#$%&’()*٪+,-/:;<=>?@[\]^_`{|}~0-9۱۲۳۴۵۶۷۸۹۰]','',review_text)
  review_text = re.sub(r'[\s]{2,}', ' ',review_text)
  review_text = re.sub(r'(\w)\1{2,}', r'\1',review_text)
  review_text = normalizer.normalize(review_text)
  # review_text = sent_tokenize(review_text)
  review_text = word_tokenize(review_text)
  review_text = [item for item in review_text if item not in stopwords]
  review_text = [stemmer.stem(i) for i in review_text]
  review_text = [lemma.lemmatize(word=w, pos='v') for w in review_text]
  review_text = [i for i in review_text if len(i) > 1]
  review_text = ' '.join(review_text)
  if re.search(r'[\u0600-\u06FF]', review_text):
    return review_text
  else:
    return None

tqdm.pandas()

df_ac.isnull().sum()

df_ac = df_ac.dropna()

df_ac.info()

df_ac['CleanText'] = df_ac['comment'].progress_apply(preprocess)

df_ac['CleanText'].iloc[11]

df_ac.head(5)

df_ac = df_ac.dropna()

df_ac.info()

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform the 'category' column
df_ac['recommend'] = label_encoder.fit_transform(df_ac['recommend'])
# 2 = recommended 1= not_recommended 0=no_idea

df_ac.iloc[20:50:5,:]

"""## TF/IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer
tf = TfidfVectorizer(min_df=2, max_features= 7000)
X_tfidf = tf.fit_transform(df_ac['CleanText']).todense()

labels = df_ac['recommend'].values

X_tfidf = np.squeeze(np.asarray(X_tfidf))

#balancing data
from imblearn.over_sampling import SMOTE

# X is your text data and y is the corresponding labels
smote = SMOTE()
X, y = smote.fit_resample(X_tfidf, labels)

"""## train test split"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

import warnings
warnings.filterwarnings("ignore")

"""## Logistic regression"""

from sklearn.model_selection import GridSearchCV

# hyperparameters tuning
# LR = LogisticRegression()
# grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],
#         'penalty': ['l1', 'l2'],
#         'solver': ['lbfgs', 'liblinear']}

# # perform grid search cross-validation to find the best hyperparameters
# grid_search = GridSearchCV(estimator=LR, param_grid=grid, cv=5, n_jobs=-1, scoring='accuracy',verbose=4)
# grid_search.fit(X_train, y_train)

# # print the best hyperparameters and the corresponding mean cross-validation score
# print("Best Hyperparameters: ", grid_search.best_params_)
# print("Best Accuracy Score: ", grid_search.best_score_)

#logisticregression
from time import perf_counter
start_tra = perf_counter()

LR = LogisticRegression(C=10, penalty='l2', solver='liblinear', random_state=42)
LR.fit(X_train,y_train)

end_tra = perf_counter()
print(f'train phase time: ', round((end_tra-start_tra), 1))

score = LR.score(X_test, y_test)
print("Accuracy:", score)
y_pred = LR.predict(X_test)
print(classification_report(y_test, y_pred))

"""## Naive bayes"""

# hyperparameters tuning
# params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0],
#           'fit_prior': [True, False],
#          }

# nb_grid = GridSearchCV(MultinomialNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)
# nb_grid.fit(X_train,y_train)

# print('Best Parameters : {}'.format(nb_grid.best_params_))
# print('Best Accuracy Through Grid Search : {:.3f}\n'.format(nb_grid.best_score_))

start_tra = perf_counter()

NB = MultinomialNB(alpha=0.5, fit_prior= True)
NB.fit(X_train,y_train)

end_tra = perf_counter()
print(f'train phase time: ', round((end_tra-start_tra), 1))


score = NB.score(X_test, y_test)
print("Accuracy:", score)
y_pred_NB = NB.predict(X_test)
print(classification_report(y_test, y_pred_NB))

"""## testing model"""

# LogisticRegression
X_pred = tf.transform([preprocess('چقددد خفنننن بود')]).todense()
X_pred = np.asarray(X_pred)
y_new = LR.predict(X_pred)
if y_new[0] == 1:
  print('Positive')
else:
  print('Negative')

# Naive Bayes
X_pred = tf.transform([preprocess('خیلی بد بود')]).todense()
X_pred = np.asarray(X_pred)
y_new = NB.predict(X_pred)
if y_new[0] == 1:
  print('Positive')
else:
    print('Negative')